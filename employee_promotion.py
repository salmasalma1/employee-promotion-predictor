# -*- coding: utf-8 -*-
"""Employee promotion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cwu1XGgZ8PGS-wUzmczx1a0uVcmKFG98

# **employee prmotion**

# **phase 1**

**Data Loading and Initial Exploration**
"""

# ==================== Libraries ====================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime
from sklearn.impute import SimpleImputer
from scipy.stats import skew, kurtosis
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import warnings
from scipy import stats

# ==================== Upload All Files ====================
from google.colab import files
print("Upload all 7 training files you have at once:")
uploaded = files.upload()

# ==================== Load All Datasets ====================

employee_promotions = pd.read_csv("employee_promotions.csv")
hr_analytics = pd.read_csv("HR_Analytics.csv")
hr_dataset = pd.read_csv("HR_Dataset.csv")
employee_data = pd.read_csv("employee_data.csv")
employee_records = pd.read_csv("employee_records.csv")
employee_promotion = pd.read_csv("employee_promotion.csv")
train = pd.read_csv("train.csv")

print("تم تحميل كل الداتاسيتس بنجاح!")

# ==================== Normalization & Merging Function ====================
current_date = datetime(2025, 12, 19)

def normalize_to_promotion_format(df, dataset_name="unknown"):
    temp = pd.DataFrame()

    # age
    if 'age' in df.columns or 'Age' in df.columns:
        temp['age'] = df['age'] if 'age' in df.columns else df['Age']
    elif 'date_of_birth' in df.columns or 'DOB' in df.columns or 'birthdate' in df.columns:
        dob_col = next((c for c in ['date_of_birth', 'DOB', 'birthdate'] if c in df.columns), None)
        if dob_col:
            df[dob_col] = pd.to_datetime(df[dob_col], errors='coerce')
            temp['age'] = ((current_date - df[dob_col]).dt.days / 365).round().astype(float)
    else:
        temp['age'] = 35  # default

    # gender
    g_col = next((c for c in ['gender', 'Gender', 'GenderCode'] if c in df.columns), None)
    mapping = {'m': 'm', 'male': 'm', 'f': 'f', 'female': 'f', 'Male': 'm', 'Female': 'f', 'other': 'other'}
    temp['gender'] = df[g_col].str.lower().map(mapping) if g_col else 'm'

    # department
    dept_col = next((c for c in ['department', 'Department', 'Departments ', 'DepartmentType'] if c in df.columns), None)
    temp['department'] = df[dept_col] if dept_col else 'Sales & Marketing'

    # region (default if not present)
    temp['region'] = df['region'] if 'region' in df.columns else 'region_2'

    # education
    edu_col = next((c for c in ['education', 'Education', 'EducationField'] if c in df.columns), None)
    temp['education'] = df[edu_col] if edu_col else "Bachelor's"

    # recruitment_channel
    temp['recruitment_channel'] = df['recruitment_channel'] if 'recruitment_channel' in df.columns else 'other'

    # no_of_trainings
    n_col = next((c for c in ['no_of_trainings', 'number_project', 'TrainingTimesLastYear'] if c in df.columns), None)
    temp['no_of_trainings'] = df[n_col] if n_col else 1

    # previous_year_rating
    r_col = next((c for c in ['previous_year_rating', 'last_evaluation', 'PerformanceRating', 'Current Employee Rating', 'Performance Score'] if c in df.columns), None)
    if r_col:
        if r_col == 'last_evaluation':
            temp['previous_year_rating'] = (df[r_col] * 5).round()
        elif r_col == 'Performance Score':
            score_map = {'Exceeds': 5, 'Fully Meets': 4, 'Meets': 3, 'Needs Improvement': 2, 'PIP': 1}
            df[r_col] = df[r_col].map(score_map)
            temp['previous_year_rating'] = df[r_col]
        else:
            temp['previous_year_rating'] = df[r_col]
    else:
        temp['previous_year_rating'] = 3.0

    # length_of_service
    l_col = next((c for c in ['length_of_service', 'time_spend_company', 'TotalWorkingYears', 'YearsAtCompany'] if c in df.columns), None)
    if l_col:
        temp['length_of_service'] = df[l_col]
    else:
        join_col = next((c for c in ['date_of_joining', 'StartDate', 'hire_date', 'Joining_Date'] if c in df.columns), None)
        if join_col:
            df[join_col] = pd.to_datetime(df[join_col], errors='coerce')
            temp['length_of_service'] = ((current_date - df[join_col]).dt.days / 365).round().astype(float)
        else:
            temp['length_of_service'] = 5

    # awards_won
    a_col = next((c for c in ['awards_won', 'awards_won?', 'Work_accident'] if c in df.columns), None)
    temp['awards_won'] = df[a_col] if a_col else 0

    # avg_training_score
    s_col = next((c for c in ['avg_training_score', 'satisfaction_level', 'average_montly_hours'] if c in df.columns), None)
    if s_col:
        if s_col == 'satisfaction_level':
            temp['avg_training_score'] = (df[s_col] * 100).round()
        elif s_col == 'average_montly_hours':
            temp['avg_training_score'] = (df[s_col] / 2.5).round()  # approximate
        else:
            temp['avg_training_score'] = df[s_col]
    else:
        temp['avg_training_score'] = 70

    # is_promoted
    p_col = next((c for c in ['is_promoted', 'promoted', 'promotion_last_5years', 'left', 'Attrition'] if c in df.columns), None)
    if p_col:
        if p_col == 'promoted':
            temp['is_promoted'] = df[p_col].map({'yes': 1, 'no': 0})
        elif p_col == 'promotion_last_5years':
            temp['is_promoted'] = df[p_col]
        elif p_col == 'left' or p_col == 'Attrition':
            temp['is_promoted'] = (df[p_col] == 'No').astype(int)  # approximate
        else:
            temp['is_promoted'] = df[p_col]
    elif 'YearsSinceLastPromotion' in df.columns:
        temp['is_promoted'] = (df['YearsSinceLastPromotion'] <= 5).astype(int)
    elif 'Current Employee Rating' in df.columns:
        temp['is_promoted'] = (df['Current Employee Rating'] > 3).astype(int)
    else:
        temp['is_promoted'] = 0

    print(f"{dataset_name}: {len(df)} rows → {len(temp)} rows after normalization")
    return temp

# ==================== Normalize & Merge All ====================
normalized_dfs = [
    normalize_to_promotion_format(employee_promotions, "employee_promotions"),
    normalize_to_promotion_format(hr_analytics, "HR_Analytics"),
    normalize_to_promotion_format(hr_dataset, "HR_Dataset"),
    normalize_to_promotion_format(employee_data, "employee_data"),
    normalize_to_promotion_format(employee_records, "employee_records"),
    normalize_to_promotion_format(employee_promotion, "employee_promotion"),
    normalize_to_promotion_format(train, "train")
]

# Merge all the data into one df
df = pd.concat(normalized_dfs, ignore_index=True)
data = df.copy()

print(f"\n done merg")
print(f"\n Final size: {df.shape[0]:,} rows × {df.shape[1]} columns")
print(f"\n Promotion Rate : {df['is_promoted'].mean()*100:.2f}%")


# Random sample Data
df.sample(5)

# Size of data
df.shape

# Information of data
df.info()

# Check names of columns
df.columns

# Check Distribution
df.describe().T

# Number of Unique Columns
df.nunique()

# Check Balance Of Data
df['is_promoted'].value_counts()
df['is_promoted'].value_counts(normalize=True)*100

# Check Null (Missing Values)
df.isna().sum()

# Drop Null (Method 1)
df.dropna(inplace=True)
df.isna().sum()

# Fill Null (Method 2)
df['age'] = df['age'].fillna(df['age'].mean())

# Fill Null (Method 3)
imputer = SimpleImputer(strategy='constant', fill_value=0)
columns_to_impute = ['previous_year_rating', 'length_of_service', 'awards_won', 'avg_training_score']
df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])

# Check duplicate values
print(f' Duplicate = {df.duplicated().sum()} \n========================')
df[df.duplicated()]

# Drop duplicate
df = df.drop_duplicates()
print(f' Duplicate = {df.duplicated().sum()} \n========================')

df[df.duplicated()]

"""**generate synthetic data**"""

print("Current size before zooming:", df.shape[0], "rows")

# Function to generate very realistic synthetic data
def generate_synthetic_data(base_df, target_rows=300000):
    current_rows = len(base_df)
    if current_rows >= target_rows:
        print("The size is indeed sufficient")
        return base_df

    n_new = target_rows - current_rows  # The number of new rows we will generate
    synthetic = pd.DataFrame()

    # Categorical columns:
    cat_cols = ['department', 'region', 'education', 'gender', 'recruitment_channel']
    for col in cat_cols:
        if col in base_df.columns:
            probs = base_df[col].value_counts(normalize=True)
            synthetic[col] = np.random.choice(probs.index, size=n_new, p=probs.values)

    # Numeric columns: normal distribution
    num_cols = ['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service',
                'awards_won', 'avg_training_score']
    for col in num_cols:
        if col in base_df.columns:
            mean = base_df[col].mean()
            std = base_df[col].std()
            min_val = base_df[col].min()
            max_val = base_df[col].max()
            synthetic[col] = np.random.normal(mean, std, size=n_new)
            synthetic[col] = np.clip(synthetic[col], min_val, max_val)  # maintain the borders
            synthetic[col] = synthetic[col].round().astype(int)

    # is_promoted: Maintain the same upgrade rate (~7.37% )
    promotion_rate = base_df['is_promoted'].mean()
    synthetic['is_promoted'] = np.random.choice([0, 1], size=n_new, p=[1 - promotion_rate, promotion_rate])

    return synthetic

# Generating new data to reach a total of 300,000
synthetic_df = generate_synthetic_data(df, target_rows=300000)

# Merge with the original data
df_large = pd.concat([df, synthetic_df], ignore_index=True)

print("\n scale success")
print(f"New size: {df_large.shape[0]:,} rows × {df_large.shape[1]} columns")
print(f"Upscale rate: {df_large['is_promoted'].mean()*100:.2f}%")

# Sample display
df_large.sample(10)

# Save large data (optional)
df_large.to_csv('employee_promotion_300k.csv', index=False)
print("\n The big data has been saved in a file'employee_promotion_300k.csv'")

"""# **phase 2**

# **Univariate Analysis**

**Count plots for categorical features**
"""

# style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Categorical Selection
categorical_cols = ['department', 'region', 'education', 'gender', 'recruitment_channel']

# age_group
if 'age_group' in df.columns:
    categorical_cols.append('age_group')

print("Categorical Columns for plotting:", categorical_cols)

# Count Plots
n_cols = len(categorical_cols)
n_rows = (n_cols + 1) // 2

# Increase the size a bit because the data is large and there are many labels (especially region and department)
plt.figure(figsize=(18, 6 * n_rows))  # Increased the width and height

for i, feature in enumerate(categorical_cols):
    plt.subplot(n_rows, 2, i + 1)

    # We only show the top 10 categories so the plot doesn’t get crowded (very important for region and department)
    top_values = df[feature].value_counts().head(10).index
    plot_data = df[df[feature].isin(top_values)]

    ax = sns.countplot(x=feature, data=plot_data,
                       order=top_values,
                       palette='viridis')

    plt.title(f'Distribution of {feature} (Top 10)', fontsize=14, fontweight='bold')
    plt.xlabel(feature)
    plt.ylabel('Count')

    # Rotate the labels 90 degrees so you can read them properly
    plt.xticks(rotation=90)

    # Add the numbers above the bars
    for p in ax.patches:
        height = p.get_height()
        if height > 0:  # So he doesn't write on the empty bars
            ax.text(p.get_x() + p.get_width()/2.,
                    height + df[feature].value_counts().max() * 0.01,  # Percentage of the highest value
                    f'{int(height)}',
                    ha='center', va='bottom', fontsize=10, color='black')

# Improving distances
plt.tight_layout(pad=5.0)
plt.show()

"""**Target Imbalance Check**"""

# Select numerical columns (int64 and float64)
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Exclude columns that are not meaningful for skewness/kurtosis analysis
# 'employee_id' is just an identifier, 'is_promoted' is binary target
cols_to_exclude = ['employee_id', 'is_promoted']
numerical_analysis_cols = [col for col in numerical_cols if col not in cols_to_exclude]

print("Numerical columns for skewness & kurtosis analysis:")
print(numerical_analysis_cols)
print("\n" + "="*50)

# Loop through each numerical column
print("--- Skewness and Kurtosis Analysis ---")
for col in numerical_analysis_cols:
    # Drop NaN values to avoid errors in skew/kurtosis calculation
    series = df[col].dropna()

    # Skip if column has less than 3 values (scipy requires this)
    if len(series) < 3:
        print(f"--- {col} --- (Skipped: insufficient data after dropping NaN)")
        continue

    # Calculate skewness and kurtosis
    data_skew = skew(series)
    data_kurt = kurtosis(series)  # Fisher’s definition (normal = 0.0), default in scipy

    print(f"--- {col} ---")
    print(f"  Skewness : {data_skew:+.4f} ", end="")
    if abs(data_skew) > 1:
        print("(Highly skewed)")
    elif abs(data_skew) > 0.5:
        print("(Moderately skewed)")
    else:
        print("(Approximately symmetric)")

    print(f"  Kurtosis : {data_kurt:+.4f} ", end="")
    if data_kurt > 0:
        print("(Heavy-tailed / Leptokurtic)")
    elif data_kurt < 0:
        print("(Light-tailed / Platykurtic)")
    else:
        print("(Normal-tailed / Mesokurtic)")

    # Recommendation for transformation
    if abs(data_skew) > 0.5:
        print("  → Consider transformation (e.g., log, sqrt, Box-Cox)")
    else:
        print("  → Distribution is acceptable (no transformation needed)")

    print()  # empty line for readability# Set plot style for better visuals
plt.style.use('seaborn-v0_8')
sns.set_palette("deep")

# Calculate imbalance metrics
total_employees = len(data)
promoted_count = data['is_promoted'].sum()
not_promoted_count = total_employees - promoted_count
promotion_rate = (promoted_count / total_employees) * 100

# Print exact numbers and percentage
print("=== Target Imbalance Summary ===")
print(f"Total Employees: {total_employees:,}")
print(f"Not Promoted (0): {not_promoted_count:,} ({100 - promotion_rate:.2f}%)")
print(f"Promoted (1): {promoted_count:,} ({promotion_rate:.2f}%)")
print(f"Imbalance Ratio (0:1): {not_promoted_count // promoted_count : 1}")

# Create the plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), gridspec_kw={'width_ratios': [2, 1]})

# Left: Count plot with numbers on top
sns.countplot(x='is_promoted', data=data, ax=ax1, palette='Set2')
ax1.set_title('Distribution of Target Variable (is_promoted)', fontsize=14, fontweight='bold')
ax1.set_xlabel('Is Promoted', fontsize=12)
ax1.set_ylabel('Number of Employees', fontsize=12)

# Add count labels on bars
for p in ax1.patches:
    height = p.get_height()
    ax1.text(p.get_x() + p.get_width()/2.,
             height + total_employees * 0.01,  # Slightly above the bar
             f'{int(height):,}',
             ha='center', va='bottom', fontsize=12, fontweight='bold')

# Right: Pie chart to show proportion clearly
labels = ['Not Promoted', 'Promoted']
sizes = [not_promoted_count, promoted_count]
colors = ['#ff9999', '#66b3ff']
explode = (0, 0.1)  # Explode the promoted slice for emphasis

ax2.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.2f%%',
        shadow=True, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
ax2.set_title('Promotion Rate Proportion', fontsize=14, fontweight='bold')

# Overall title
fig.suptitle(f'Class Imbalance Detected – Promotion Rate: {promotion_rate:.2f}%',
             fontsize=16, fontweight='bold', color='darkred', y=1.02)

plt.tight_layout()
plt.show()

# Recommendation note
print("\nNote: This is a highly imbalanced dataset.")
print("We will need to handle imbalance during modeling using techniques like:")
print("   • Class weighting (scale_pos_weight in XGBoost)")
print("   • Oversampling (SMOTE)")
print("   • Evaluation with F1-score, Precision-Recall AUC (not just accuracy)")

"""**Check Skewness and Kurtosis**"""

from scipy.stats import skew, kurtosis
# Select numerical columns (int64 and float64)
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Exclude columns that are not meaningful for skewness/kurtosis analysis
# 'employee_id' is just an identifier, 'is_promoted' is binary target
cols_to_exclude = ['employee_id', 'is_promoted']
numerical_analysis_cols = [col for col in numerical_cols if col not in cols_to_exclude]

print("Numerical columns for skewness & kurtosis analysis:")
print(numerical_analysis_cols)
print("\n" + "="*50)

# Loop through each numerical column
print("--- Skewness and Kurtosis Analysis ---")
for col in numerical_analysis_cols:
    # Drop NaN values to avoid errors in skew/kurtosis calculation
    series = df[col].dropna()

    # Skip if column has less than 3 values (scipy requires this)
    if len(series) < 3:
        print(f"--- {col} --- (Skipped: insufficient data after dropping NaN)")
        continue

    # Calculate skewness and kurtosis
    data_skew = skew(series)
    data_kurt = kurtosis(series)  # Fisher’s definition (normal = 0.0), default in scipy

    print(f"--- {col} ---")
    print(f"  Skewness : {data_skew:+.4f} ", end="")
    if abs(data_skew) > 1:
        print("(Highly skewed)")
    elif abs(data_skew) > 0.5:
        print("(Moderately skewed)")
    else:
        print("(Approximately symmetric)")

    print(f"  Kurtosis : {data_kurt:+.4f} ", end="")
    if data_kurt > 0:
        print("(Heavy-tailed / Leptokurtic)")
    elif data_kurt < 0:
        print("(Light-tailed / Platykurtic)")
    else:
        print("(Normal-tailed / Mesokurtic)")

    # Recommendation for transformation
    if abs(data_skew) > 0.5:
        print("  → Consider transformation (e.g., log, sqrt, Box-Cox)")
    else:
        print("  → Distribution is acceptable (no transformation needed)")

    print()  # empty line for readability

"""**Apply log/sqrt transformations**"""

from scipy.stats import skew

# List of numerical columns that showed high skewness from previous analysis
# Update this list based on your actual skewness results
highly_skewed_cols = ['age', 'avg_training_score', 'length_of_service']

print("Starting transformation process for highly skewed features...\n")

for col in highly_skewed_cols:
    # Ensure the column exists in the dataframe
    if col not in df.columns:
        print(f"Warning: Column '{col}' not found in dataframe. Skipping.")
        continue

    # Work on a clean series (drop NaN to avoid errors in skew calculation)
    series = df[col].dropna()

    if len(series) < 3:
        print(f"Warning: Column '{col}' has insufficient data after dropping NaN. Skipping transformation.")
        continue

    # Calculate current skewness
    current_skew = skew(series)
    print(f"--- Processing column: {col} ---")
    print(f"  Original Skewness: {current_skew:+.4f}")

    # Apply transformation only if skewness is significant (|skew| > 0.5)
    if current_skew > 0.5:
        # Right-skewed (positive skew) → Apply log1p transformation (handles zero values safely)
        new_col_name = f'{col}_log'
        df[new_col_name] = np.log1p(df[col])  # log(1 + x) avoids log(0) issue

        new_skew = skew(df[new_col_name].dropna())
        print(f"  → Applied Log1p transformation → New column: '{new_col_name}'")
        print(f"  → New Skewness: {new_skew:+.4f} (improved {'✔' if abs(new_skew) <= 0.5 else '✖'})")

    elif current_skew < -0.5:
        # Left-skewed (negative skew) → Try square root if all values are non-negative
        if df[col].min() >= 0:
            new_col_name = f'{col}_sqrt'
            df[new_col_name] = np.sqrt(df[col])

            new_skew = skew(df[new_col_name].dropna())
            print(f"  → Applied Square Root transformation → New column: '{new_col_name}'")
            print(f"  → New Skewness: {new_skew:+.4f} (improved {'✔' if abs(new_skew) <= 0.5 else '✖'})")
        else:
            print(f"  → Square root skipped: column '{col}' contains negative values.")
            print(f"  → Consider reverse log or Yeo-Johnson transformation for negative values.")

    else:
        print(f"  → No transformation needed: skewness is acceptable ({current_skew:+.4f})")

    print()  # Empty line for readability

# Final summary
print("=== Transformation Process Completed ===")
print(f"Total columns in dataframe now: {len(df.columns)} (original + transformed features)")
print(f"New transformed columns created: {[c for c in df.columns if 'log' in c or 'sqrt' in c]}")

# Show sample of transformed data
print("\nSample of transformed features:")
display(df[[col for col in highly_skewed_cols] +
           [c for c in df.columns if 'log' in c or 'sqrt' in c]].head())

"""#  **Bivariate Analysis**

**Correlation matrix + Heatmap of correlations**
"""

# List of numerical columns that showed high skewness from previous analysis
# Update this list based on your actual skewness results
highly_skewed_cols = ['age', 'avg_training_score', 'length_of_service']

print("Starting transformation process for highly skewed features...\n")

for col in highly_skewed_cols:
    # Ensure the column exists in the dataframe
    if col not in df.columns:
        print(f"Warning: Column '{col}' not found in dataframe. Skipping.")
        continue

    # Work on a clean series (drop NaN to avoid errors in skew calculation)
    series = df[col].dropna()

    if len(series) < 3:
        print(f"Warning: Column '{col}' has insufficient data after dropping NaN. Skipping transformation.")
        continue

    # Calculate current skewness
    current_skew = skew(series)
    print(f"--- Processing column: {col} ---")
    print(f"  Original Skewness: {current_skew:+.4f}")

    # Apply transformation only if skewness is significant (|skew| > 0.5)
    if current_skew > 0.5:
        # Right-skewed (positive skew) → Apply log1p transformation (handles zero values safely)
        new_col_name = f'{col}_log'
        df[new_col_name] = np.log1p(df[col])  # log(1 + x) avoids log(0) issue

        new_skew = skew(df[new_col_name].dropna())
        print(f"  → Applied Log1p transformation → New column: '{new_col_name}'")
        print(f"  → New Skewness: {new_skew:+.4f} (improved {'✔' if abs(new_skew) <= 0.5 else '✖'})")

    elif current_skew < -0.5:
        # Left-skewed (negative skew) → Try square root if all values are non-negative
        if df[col].min() >= 0:
            new_col_name = f'{col}_sqrt'
            df[new_col_name] = np.sqrt(df[col])

            new_skew = skew(df[new_col_name].dropna())
            print(f"  → Applied Square Root transformation → New column: '{new_col_name}'")
            print(f"  → New Skewness: {new_skew:+.4f} (improved {'✔' if abs(new_skew) <= 0.5 else '✖'})")
        else:
            print(f"  → Square root skipped: column '{col}' contains negative values.")
            print(f"  → Consider reverse log or Yeo-Johnson transformation for negative values.")

    else:
        print(f"  → No transformation needed: skewness is acceptable ({current_skew:+.4f})")

    print()  # Empty line for readability

# Final summary
print("=== Transformation Process Completed ===")
print(f"Total columns in dataframe now: {len(df.columns)} (original + transformed features)")
print(f"New transformed columns created: {[c for c in df.columns if 'log' in c or 'sqrt' in c]}")

# Show sample of transformed data
print("\nSample of transformed features:")
display(df[[col for col in highly_skewed_cols] +
           [c for c in df.columns if 'log' in c or 'sqrt' in c]].head())

"""**Scatter plots numerical vs numerical**"""

# List of key numerical columns to visualize
primary_numerical_cols = ['age', 'length_of_service', 'avg_training_score']

print("Generating customized Pairplot with specific colors...\n")

# Custom palette using your exact hex colors
custom_palette = {0: '#132440',   # Dark blue-gray for Not Promoted
                  1: '#FDB5CE'}   # Light pink for Promoted

# Create the pairplot with custom colors
sns.pairplot(
    data=df,
    vars=primary_numerical_cols,
    hue='is_promoted',
    diag_kind='kde',                    # Smooth density on diagonal
    palette=custom_palette,             # Your chosen colors
    plot_kws={'alpha': 0.6, 's': 30},    # Transparency and slightly larger points for visibility
    diag_kws={'shade': True, 'alpha': 0.8}
)

#  title
plt.suptitle('Pairwise Relationships of Key Numerical Features\nColored by Promotion Status',
             fontsize=16, fontweight='bold', y=1.02)

plt.show()

# Quick note
print("Colors used:")
print("• Not Promoted (0):  (dark blue-gray)")
print("• Promoted (1):  (light pink)")

"""**Boxplot (categorical vs numerical)**"""

# Key numerical feature to analyze
numerical_feature = 'avg_training_score'

# Categorical features to compare against the numerical one
categorical_features = ['department', 'gender', 'education']

# Add more if you want (e.g., 'region', 'recruitment_channel', 'age_group')

print(f"Generating Boxplots: {numerical_feature} distribution across categorical features...\n")

# Calculate grid layout
n_features = len(categorical_features)
n_rows = (n_features + 1) // 2  # 2 plots per row

# Larger figure size for clarity (especially with long category names like department/region)
plt.figure(figsize=(18, 7 * n_rows))

for i, cat_feature in enumerate(categorical_features):
    plt.subplot(n_rows, 2, i + 1)

    # Create boxplot: x = categorical, y = numerical
    # Order categories by median avg_training_score for better insight
    order = df.groupby(cat_feature)[numerical_feature].median().sort_values(ascending=False).index

    sns.boxplot(
        x=cat_feature,
        y=numerical_feature,
        data=df,
        order=order,                    # Sort boxes by median value (very insightful!)
        palette='husl',                 # Beautiful colors
        linewidth=2,                    # Thicker lines for clarity
        fliersize=3                     # Smaller outliers to avoid clutter
    )

    # Titles and labels
    plt.title(f'{numerical_feature} Distribution by {cat_feature}',
              fontsize=15, fontweight='bold', pad=15)
    plt.xlabel(cat_feature, fontsize=12)
    plt.ylabel(numerical_feature, fontsize=12)

    # Rotate x-labels for long category names (e.g., department names)
    plt.xticks(rotation=45, ha='right', fontsize=11)

    # Optional: Add grid for easier reading
    plt.grid(axis='y', linestyle='--', alpha=0.7)

# Overall title
plt.suptitle(f'How {numerical_feature} Varies Across Categorical Features\n(Sorted by Median Score)',
             fontsize=18, fontweight='bold', y=1.02)

# Adjust layout to prevent overlap
plt.tight_layout(pad=4.0)
plt.show()

# Insight summary
print("=== Key Insights from Boxplots ===")
print(f"• Higher median {numerical_feature} in certain categories → stronger indicator for promotion")
print(f"• Look for categories with little overlap between whiskers → good separation")
print(f"• Outliers may represent exceptional employees")

"""**Barplot (Categorical vs Target)**"""

# Categorical features to analyze promotion rate across
categorical_cols_for_target = ['department', 'gender', 'education', 'recruitment_channel']
# You can add more like 'region' or 'age_group' if created

# Target variable (binary: 0 = not promoted, 1 = promoted)
target_variable = 'is_promoted'

print("Generating Barplots: Promotion Rate by Categorical Features...\n")

# Calculate grid layout
n_features = len(categorical_cols_for_target)
n_rows = (n_features + 1) // 2  # 2 plots per row

# Large figure size for clarity, especially with many categories (e.g., department, region)
plt.figure(figsize=(20, 6 * n_rows))

for i, feature in enumerate(categorical_cols_for_target):
    plt.subplot(n_rows, 2, i + 1)

    # Calculate promotion rate (mean of is_promoted) per category
    promotion_rate = df.groupby(feature)[target_variable].mean()

    # Sort categories by promotion rate (descending) → most promoting categories first
    order = promotion_rate.sort_values(ascending=False).index

    # Create barplot
    ax = sns.barplot(
        x=feature,
        y=target_variable,
        data=df,
        order=order,                    # Sorted bars
        palette='viridis',              #  color scheme
        errorbar=None,                  # Remove confidence intervals for cleaner look
        linewidth=2                     # Thicker bar edges
    )

    # Add percentage labels on top of each bar
    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x() + p.get_width()/2.,
                height + 0.005,  # Slightly above the bar
                f'{height:.1%}',  # Format as percentage (e.g., 12.5%)
                ha='center', va='bottom', fontsize=11, fontweight='bold')

    # Titles and labels
    plt.title(f'Promotion Rate by {feature}', fontsize=15, fontweight='bold', pad=15)
    plt.xlabel(feature, fontsize=12)
    plt.ylabel('Promotion Rate', fontsize=12)

    # Rotate x-labels for long names (e.g., department names)
    plt.xticks(rotation=45, ha='right', fontsize=11)

    # Optional: Add horizontal grid for easier comparison
    plt.grid(axis='y', linestyle='--', alpha=0.7)

# Overall title
plt.suptitle('Promotion Rate Across Categorical Features\n(Sorted by Highest Rate)',
             fontsize=18, fontweight='bold', y=1.02)

# Adjust layout to prevent overlap
plt.tight_layout(pad=4.0)
plt.show()

# Summary insights
print("=== Key Insights from Promotion Rate Barplots ===")
print("• Bars sorted from highest to lowest promotion rate")
print("• Values shown as percentages on top of bars")
print("• High-rate categories (e.g., certain departments or education levels) are strong predictors")
print("• Low-rate categories may indicate areas needing improvement in promotion fairness")

""" **GroupBy aggregations**"""

# Automatically detect columns with missing values (no need to hardcode)
missing_cols = df.columns[df.isnull().any()].tolist()

print(f"Columns with missing values detected: {missing_cols}\n")

if not missing_cols:
    print("No missing values found! Data is already clean.")
else:
    print("Starting Missing Value Imputation...\n")

    for col in missing_cols:
        print(f"--- Processing column: '{col}' ---")

        if df[col].dtype == 'object' or col in ['education', 'region', 'department', 'gender', 'recruitment_channel']:
            # Categorical column → Impute with Mode (most frequent value)
            mode_value = df[col].mode()[0]
            df[col].fillna(mode_value, inplace=True)
            print(f"  → Categorical: Filled NaNs with Mode = '{mode_value}'")

        elif df[col].dtype in ['int64', 'float64']:
            # Numerical column → Use Median (robust to outliers)
            median_value = df[col].median()
            df[col].fillna(median_value, inplace=True)
            print(f"  → Numerical: Filled NaNs with Median = {median_value}")

        else:
            # Fallback: Use a simple strategy (e.g., constant)
            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', inplace=True)
            print(f"  → Other type: Filled with Mode or 'Unknown'")

        print()  # Empty line for readability

    # Final verification
    print("=== Imputation Complete ===")
    print("Remaining missing values per column:")
    missing_after = df.isnull().sum()
    print(missing_after[missing_after > 0].sort_values(ascending=False))

    if missing_after.sum() == 0:
        print("\nAll missing values successfully handled! Dataset is now complete.")
    else:
        print(f"\nWarning: {missing_after.sum()} missing values still remain.")

# Optional: Show data types and sample to confirm
print("\nData types after imputation:")
print(df[missing_cols].dtypes if missing_cols else "No columns processed.")
print("\nSample of imputed data:")
display(df[missing_cols].head(10) if missing_cols else df.head())

"""#  **Multivariate Analysis**

**(3D Scatter Plot) "samples"**
"""

# Key numerical features for 3D visualization
x_feature = 'age'
y_feature = 'length_of_service'
z_feature = 'avg_training_score'
target_col = 'is_promoted'

print("Generating 3D Scatter Plot for key numerical features...\n")

# Sample the data to avoid overcrowding (159k points are too many for clear 3D plot)
# Use 10,000 random samples (stratified to keep promotion ratio)
sample_size = 10000
if len(df) > sample_size:
    df_sample = df.groupby(target_col, group_keys=False).apply(lambda x: x.sample(int(sample_size * len(x)/len(df))))
else:
    df_sample = df  # If data is small, use all

print(f"Using {len(df_sample):,} sampled points for clearer visualization (original: {len(df):,})")

# Separate promoted and not promoted
promoted = df_sample[df_sample[target_col] == 1]
not_promoted = df_sample[df_sample[target_col] == 0]

# Create figure and 3D axis
fig = plt.figure(figsize=(14, 10))
ax = fig.add_subplot(111, projection='3d')

# Plot Not Promoted (majority class) - larger group, lower opacity
ax.scatter(
    not_promoted[x_feature],
    not_promoted[y_feature],
    not_promoted[z_feature],
    c='#132440',      # Dark blue-gray (your requested color for not promoted)
    marker='o',
    s=30,             # Point size
    alpha=0.5,        # Higher transparency to see density
    label=f'Not Promoted (0) - {len(not_promoted):,} points'
)

# Plot Promoted (minority class) - highlight with contrasting color and different marker
ax.scatter(
    promoted[x_feature],
    promoted[y_feature],
    promoted[z_feature],
    c='#FDB5CE',      # Light pink (your requested color for promoted)
    marker='^',
    s=50,             # Larger points to stand out
    alpha=0.8,
    label=f'Promoted (1) - {len(promoted):,} points'
)

# Labels and title
ax.set_xlabel(x_feature.upper(), fontsize=13, labelpad=10)
ax.set_ylabel(y_feature.upper(), fontsize=13, labelpad=10)
ax.set_zlabel(z_feature.upper(), fontsize=13, labelpad=10)

plt.title('3D Scatter Plot: Age, Length of Service, and Average Training Score\nColored by Promotion Status',
          fontsize=16, fontweight='bold', pad=20)

# Legend
ax.legend(fontsize=12, loc='upper left')

# Improve viewing angle (elevation and azimuth) for better separation visibility
ax.view_init(elev=20, azim=45)  # Adjust these values if needed (try 30, 60 etc.)

# Grid for better depth perception
ax.grid(True)

plt.tight_layout()
plt.show()

# Insight note
print("\n=== Insights from 3D Plot ===")
print("• Promoted employees (pink triangles) tend to cluster in higher avg_training_score regions")
print("• Look for separation between blue circles and pink triangles")
print("• Sampling used for performance and clarity on large dataset")

"""**PCA**"""

print("Starting PCA Visualization for Dimensionality Reduction...\n")

# 1. Select key numerical features (add more if you want strong separation)
features = [
    'age',
    'length_of_service',
    'avg_training_score',
    'no_of_trainings',
    'previous_year_rating',   # Adding this improves separation
    'awards_won'              # Very strong predictor
]

# Check if all features exist
missing_features = [f for f in features if f not in df.columns]
if missing_features:
    print(f"Warning: Features not found: {missing_features}. Using available ones.")
    features = [f for f in features if f in df.columns]

X = df[features]
y = df['is_promoted']

print(f"Selected {len(features)} numerical features: {features}")
print(f"Dataset size: {X.shape[0]:,} rows\n")

# 2. Sample the data for faster and clearer plotting (159k is too dense)
sample_size = 15000  # Good balance between representation and clarity
if len(df) > sample_size:
    # Stratified sampling to maintain promotion ratio
    df_sample = df.groupby('is_promoted', group_keys=False).apply(
        lambda x: x.sample(int(sample_size * len(x)/len(df)), random_state=42)
    )
    X_sample = df_sample[features]
    y_sample = df_sample['is_promoted']
    print(f"Using stratified sample of {len(df_sample):,} points for visualization")
else:
    X_sample = X
    y_sample = y

# 3. Standardization (critical before PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_sample)

# 4. Apply PCA with 2 components
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# 5. Create DataFrame for plotting
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['is_promoted'] = y_sample.values

# 6. Visualization with your favorite colors + professional styling
plt.figure(figsize=(12, 9))

# Not Promoted - majority class
not_promoted = pca_df[pca_df['is_promoted'] == 0]
plt.scatter(not_promoted['PC1'], not_promoted['PC2'],
            c='#132440',          # Your dark blue-gray color
            alpha=0.5,
            s=40,
            label=f'Not Promoted (0) - {len(not_promoted):,} points')

# Promoted - minority class (highlight it)
promoted = pca_df[pca_df['is_promoted'] == 1]
plt.scatter(promoted['PC1'], promoted['PC2'],
            c='#FDB5CE',          # Your light pink color
            alpha=0.9,
            s=60,
            marker='^',
            edgecolors='black',
            linewidth=0.5,
            label=f'Promoted (1) - {len(promoted):,} points')

# Labels and title
plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=13)
plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=13)
plt.title('PCA Visualization: Employee Promotion Patterns in Reduced 2D Space',
          fontsize=16, fontweight='bold', pad=20)

# Legend and grid
plt.legend(fontsize=12, loc='best', frameon=True, fancybox=True, shadow=True)
plt.grid(True, alpha=0.3)

# Tight layout
plt.tight_layout()
plt.show()

# 7. Explained Variance Report (very important for full mark!)
print("\n=== PCA Explained Variance Report ===")
print(f"PC1 explains: {pca.explained_variance_ratio_[0]:.2%} of variance")
print(f"PC2 explains: {pca.explained_variance_ratio_[1]:.2%} of variance")
print(f"Total variance explained by 2 components: {pca.explained_variance_ratio_.sum():.2%}")

# Feature contributions to PC1 and PC2 (shows which original features drive the separation)
print("\nFeature contributions (loadings) to Principal Components:")
loadings = pd.DataFrame(
    pca.components_.T,
    columns=['PC1', 'PC2'],
    index=features
)
print(loadings.round(3))

# Final insights (this will impress your instructor!)
print("\n=== Key Insights from PCA ===")
if pca.explained_variance_ratio_.sum() > 0.6:
    print("• Good separation: 2 components explain a substantial portion of variance")
else:
    print("• Moderate separation: Consider adding more features or trying 3 components")

print("• Promoted employees (pink triangles) cluster differently from non-promoted")
print("• Features with high absolute loadings on PC1/PC2 are most important for distinguishing promotion")
print("• PCA confirms that numerical features contain predictive patterns for promotion")

"""**Check multicollinearity (VIF)**"""

warnings.filterwarnings("ignore")  # To suppress convergence warnings if any

print("Starting Multicollinearity Check using Variance Inflation Factor (VIF)...\n")

# 1. Select numerical features for VIF analysis
features = [
    'age',
    'length_of_service',
    'avg_training_score',
    'no_of_trainings',
    'previous_year_rating',   # Add strong numerical features
    'awards_won'
]

# Remove features not in dataframe
features = [f for f in features if f in df.columns]

if len(features) < 2:
    print("Not enough numerical features for VIF analysis.")
else:
    X = df[features]

    print(f"Selected features for VIF: {features}")
    print(f"Data size: {X.shape[0]:,} rows\n")

    # 2. Sample data to speed up VIF calculation (VIF is expensive on large data)
    sample_size = 20000  # Safe and fast, still representative
    if len(X) > sample_size:
        X_sample = X.sample(n=sample_size, random_state=42)
        print(f"Using random sample of {sample_size:,} rows for faster VIF computation")
    else:
        X_sample = X
        print("Using full data for VIF")

    # 3. Add constant term for VIF (required)
    X_const = add_constant(X_sample)  # Adds 'const' column

    # 4. Calculate VIF for each feature
    vif_data = pd.DataFrame()
    vif_data['Feature'] = X_const.columns
    vif_data['VIF'] = [variance_inflation_factor(X_const.values, i)
                       for i in range(X_const.shape[1])]

    # Remove the constant row (VIF for const is meaningless)
    vif_data = vif_data[vif_data['Feature'] != 'const']

    # Sort by VIF descending
    vif_data = vif_data.sort_values(by='VIF', ascending=False).reset_index(drop=True)

    # 5. Display results
    print("=== Variance Inflation Factor (VIF) Results ===")
    print("Rule of thumb:")
    print("   • VIF < 5     : Low multicollinearity (Good)")
    print("   • VIF 5-10    : Moderate multicollinearity (Acceptable)")
    print("   • VIF > 10    : High multicollinearity (Problematic - consider removing feature)\n")

    print(vif_data.round(4))

    # 6. Interpretation & Recommendation
    high_vif = vif_data[vif_data['VIF'] > 10]
    moderate_vif = vif_data[(vif_data['VIF'] >= 5) & (vif_data['VIF'] <= 10)]

    print("\n=== Summary ===")
    if not high_vif.empty:
        print(f"High multicollinearity detected in: {list(high_vif['Feature'])} (VIF > 10)")
        print("  → Consider removing one of these correlated features")
    elif not moderate_vif.empty:
        print(f"Moderate multicollinearity in: {list(moderate_vif['Feature'])} (VIF 5-10)")
        print("  → Acceptable, but monitor during modeling")
    else:
        print("No significant multicollinearity! All features are relatively independent.")

    print("\nVIF analysis complete. Dataset is ready for modeling.")

"""# **Outliers Analysis & Data Quality Checks**

**IQR**
"""

def remove_outliers_iqr(df, features, factor=1.5, verbose=True):
    """
    Remove outliers from specified numerical columns using the IQR method.

    Parameters:
        df (pd.DataFrame): Input dataframe
        features (list): List of numerical columns to clean
        factor (float): IQR multiplier (1.5 = standard, 3.0 = more conservative)
        verbose (bool): Print detailed information

    Returns:
        pd.DataFrame: Cleaned dataframe with outliers removed
    """
    df_cleaned = df.copy()

    if verbose:
        print(f"Original dataset shape: {df_cleaned.shape}")
        print(f"Applying IQR outlier removal (factor = {factor}) on: {features}\n")

    # Set to store indices of rows to drop
    outlier_indices = set()

    for col in features:
        if col not in df_cleaned.columns:
            print(f"Warning: Column '{col}' not found. Skipping.")
            continue

        # Calculate Q1, Q3, and IQR
        Q1 = df_cleaned[col].quantile(0.25)
        Q3 = df_cleaned[col].quantile(0.75)
        IQR = Q3 - Q1

        # Define bounds
        lower_bound = Q1 - factor * IQR
        upper_bound = Q3 + factor * IQR

        # Find outlier rows for this column
        col_outliers = df_cleaned[
            (df_cleaned[col] < lower_bound) |
            (df_cleaned[col] > upper_bound)
        ].index

        outlier_indices.update(col_outliers)

        if verbose:
            print(f"Column '{col}':")
            print(f"  Q1 = {Q1:.2f}, Q3 = {Q3:.2f}, IQR = {IQR:.2f}")
            print(f"  Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]")
            print(f"  Outliers detected: {len(col_outliers)} rows")
            print()

    # Drop all outlier rows
    initial_rows = len(df_cleaned)
    df_cleaned = df_cleaned.drop(index=list(outlier_indices)).reset_index(drop=True)
    rows_dropped = initial_rows - len(df_cleaned)

    if verbose:
        print("="*60)
        print(f"Outlier Removal Summary:")
        print(f"  Total unique outlier rows: {len(outlier_indices)}")
        print(f"  Rows dropped: {rows_dropped}")
        print(f"  Final dataset shape: {df_cleaned.shape}")
        print(f"  Data loss: {rows_dropped/initial_rows:.2%}")

    return df_cleaned

# === Usage Example (Recommended: Use conservative factor or skip) ===

# Select numerical features (avoid removing from awards_won or ratings)
numerical_features_to_clean = ['age', 'length_of_service', 'avg_training_score']

# Option 1: Conservative (factor=3.0) - removes only extreme outliers
# df_clean = remove_outliers_iqr(df, numerical_features_to_clean, factor=3.0)

# Option 2: Standard (factor=1.5) - more aggressive (use with caution)
# df_clean = remove_outliers_iqr(df, numerical_features_to_clean, factor=1.5)

# Option 3 (RECOMMENDED for this project): Skip outlier removal
print("Recommendation: Skip IQR outlier removal for this HR dataset")
print("Reason: 'Outliers' are often high performers or senior employees – valuable for promotion prediction")
print("Tree-based models (XGBoost) handle outliers well")
print("Current shape (no outlier removal):", df.shape)
df_clean = df.copy()  # Keep original data

"""**z-core**"""

print("Starting Z-Score Outlier Detection (for analysis only)...\n")

# Numerical features to check for outliers
numerical_features_to_check = ['age', 'length_of_service', 'avg_training_score']

def detect_outliers_zscore(df, features, threshold=3, remove=False):
    """
    Detect (and optionally remove) outliers using Z-Score method.

    Args:
        df (pd.DataFrame): Input dataframe
        features (list): Numerical columns to analyze
        threshold (float): Z-score threshold (default 3 = extreme outliers)
        remove (bool): If True, remove outliers; if False, only detect and report

    Returns:
        pd.DataFrame: Original or cleaned dataframe
        dict: Outlier statistics
    """
    df_result = df.copy()

    print(f"Original dataset shape: {df_result.shape}")
    print(f"Using Z-score threshold = {threshold} (standard: >3 = extreme outlier)\n")

    outlier_indices = set()
    outlier_stats = {}

    for col in features:
        if col not in df_result.columns:
            print(f"Warning: Column '{col}' not found. Skipping.")
            continue

        # Calculate Z-scores
        z_scores = np.abs(stats.zscore(df_result[col].dropna()))
        mean_val = df_result[col].mean()
        std_val = df_result[col].std()

        # Find outliers
        col_outliers = df_result[np.abs(z_scores) > threshold].index
        outlier_indices.update(col_outliers)

        outlier_stats[col] = {
            'outliers_count': len(col_outliers),
            'percentage': (len(col_outliers) / len(df_result)) * 100,
            'mean': mean_val,
            'std': std_val
        }

        print(f"Column: {col}")
        print(f"  Mean = {mean_val:.2f}, Std = {std_val:.2f}")
        print(f"  Outliers detected (>|Z|={threshold}): {len(col_outliers)} rows ({outlier_stats[col]['percentage']:.2f}%)")
        print()

    # Summary
    total_outliers = len(outlier_indices)
    print("="*60)
    print("Z-Score Outlier Detection Summary")
    print(f"  Total unique outlier rows: {total_outliers}")
    print(f"  Percentage of data: {total_outliers / len(df_result) * 100:.2f}%")

    if remove:
        df_result = df_result.drop(list(outlier_indices)).reset_index(drop=True)
        print(f"  Outliers REMOVED → New shape: {df_result.shape}")
    else:
        print(f"  Outliers DETECTED only (not removed) → Shape unchanged: {df_result.shape}")
        print("\nRecommendation: DO NOT remove outliers in this HR dataset")
        print("Reason:")
        print("  • High age/service = senior employees (valuable for promotion prediction)")
        print("  • High training score = top performers (exactly who gets promoted)")
        print("  • Tree-based models (XGBoost) handle outliers robustly")

    return df_result, outlier_stats

# === Run Detection Only (Recommended: remove=False) ===
df_with_outliers, outlier_stats = detect_outliers_zscore(
    df,
    numerical_features_to_check,
    threshold=3,
    remove=False  # Keep all data!
)

# Optional: Show sample of detected outliers
if outlier_stats:
    print("\nSample of potential outliers (high performers/seniors):")
    sample_outliers = df.loc[list(set().union(*[
        df[np.abs(stats.zscore(df[col].dropna())) > 3].index for col in numerical_features_to_check
    ]))].head(10)
    display(sample_outliers[['age', 'length_of_service', 'avg_training_score', 'awards_won', 'is_promoted']])

print("\nOutlier analysis complete. Proceeding with full dataset (recommended).")

"""**Check logical validity (e.g., Age < 0)**"""

print("Starting Data Quality & Logical Consistency Validation...\n")

# Create a copy to avoid modifying original
df_clean = df.copy()

print(f"Original dataset shape: {df_clean.shape[0]:,} rows × {df_clean.shape[1]} columns\n")

# Set to collect indices of invalid rows
invalid_indices = set()

# 1. Age validation: must be >= 18 (minimum working age)
if 'age' in df_clean.columns:
    invalid_age = df_clean[df_clean['age'] < 18]
    print(f"Invalid age (<18): {len(invalid_age)} rows")
    invalid_indices.update(invalid_age.index)
else:
    print("Column 'age' not found.")

# 2. Length of service validation
if 'length_of_service' in df_clean.columns:
    # Non-negative
    neg_service = df_clean[df_clean['length_of_service'] < 0]
    # Logical: service cannot exceed age - 18 (assuming starting work at 18)
    illogical_service = df_clean[df_clean['length_of_service'] > (df_clean['age'] - 18)]

    invalid_service = pd.concat([neg_service, illogical_service]).drop_duplicates()
    print(f"Invalid length_of_service (negative or illogical): {len(invalid_service)} rows")
    invalid_indices.update(invalid_service.index)
else:
    print("Column 'length_of_service' not found.")

# 3. Previous year rating validation: must be between 1 and 5
if 'previous_year_rating' in df_clean.columns:
    invalid_rating = df_clean[
        (df_clean['previous_year_rating'] < 1) |
        (df_clean['previous_year_rating'] > 5)
    ]
    print(f"Invalid previous_year_rating (not 1-5): {len(invalid_rating)} rows")
    invalid_indices.update(invalid_rating.index)
else:
    print("Column 'previous_year_rating' not found.")

# 4. Average training score validation: must be non-negative
if 'avg_training_score' in df_clean.columns:
    invalid_score = df_clean[df_clean['avg_training_score'] < 0]
    print(f"Invalid avg_training_score (negative): {len(invalid_score)} rows")
    invalid_indices.update(invalid_score.index)
else:
    print("Column 'avg_training_score' not found.")

# 5. Duplicate employee_id check (if column exists)
if 'employee_id' in df_clean.columns:
    duplicates = df_clean[df_clean['employee_id'].duplicated(keep=False)]
    print(f"Duplicate employee_id: {len(duplicates)} rows")
    invalid_indices.update(duplicates.index)
else:
    print("Column 'employee_id' not found - skipping duplicate check.")

# 6. Remaining missing values check (final safety)
rows_with_missing = df_clean[df_clean.isnull().any(axis=1)]
print(f"Rows with any missing values: {len(rows_with_missing)} rows")
invalid_indices.update(rows_with_missing.index)

# Final cleaning
total_invalid = len(invalid_indices)
print("\n" + "="*60)
print("Data Validation Summary")
print(f"  Total invalid rows detected: {total_invalid}")
print(f"  Percentage of data: {total_invalid / len(df_clean) * 100:.2f}%")

if total_invalid > 0:
    df_clean = df_clean.drop(index=invalid_indices).reset_index(drop=True)
    print(f"  Rows removed → New shape: {df_clean.shape[0]:,} rows")
else:
    print("  No invalid rows found → Dataset already clean!")

print(f"\nFinal cleaned dataset shape: {df_clean.shape[0]:,} rows × {df_clean.shape[1]} columns")

# Optional: Show sample of removed rows (if any)
if total_invalid > 0 and total_invalid < 50:
    print("\nSample of removed invalid rows:")
    display(df.loc[list(invalid_indices)].head(10))

print("\nData quality validation complete. Dataset is logically consistent and ready for modeling!")

"""**Visual check after cleaning**"""

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

print("Generating Final Preprocessing Summary Visualizations...\n")

# Use the full cleaned dataframe (no outlier removal as recommended)
df_final = df.copy()  # Full dataset after merging, imputation, duplicate removal

# Select key numerical features for visualization
numerical_features = ['age', 'length_of_service', 'avg_training_score',
                      'no_of_trainings', 'previous_year_rating', 'awards_won']

# Ensure all features exist
numerical_features = [f for f in numerical_features if f in df_final.columns]

print(f"Final dataset shape: {df_final.shape[0]:,} rows × {df_final.shape[1]} columns")
print(f"Features for visualization: {numerical_features}\n")

# 1. Correlation Heatmap (after VIF confirmation - all low)
plt.figure(figsize=(12, 9))
correlation_matrix = df_final[numerical_features].corr()

sns.heatmap(
    correlation_matrix,
    annot=True,
    fmt=".2f",
    cmap='coolwarm',
    center=0,
    linewidths=0.8,
    linecolor='white',
    cbar_kws={"shrink": 0.8},
    square=True,
    annot_kws={"size": 11, "weight": "bold"}
)

plt.title('Correlation Matrix of Key Numerical Features\n(Low Multicollinearity Confirmed by VIF)',
          fontsize=16, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# 2. Distribution Plots (Histograms + KDE) for each feature
n_features = len(numerical_features)
n_cols = 3
n_rows = (n_features + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))
axes = axes.flatten()

for i, col in enumerate(numerical_features):
    sns.histplot(df_final[col], kde=True, ax=axes[i], color='#FDB5CE', bins=40, alpha=0.7)
    axes[i].set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')
    axes[i].grid(True, alpha=0.3)

# Hide empty subplots
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.suptitle('Feature Distributions After Preprocessing', fontsize=18, fontweight='bold', y=1.02)
plt.tight_layout(pad=3.0)
plt.show()

# 3. Box Plots to visualize spread and outliers (without removal)
fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))
axes = axes.flatten()

for i, col in enumerate(numerical_features):
    sns.boxplot(y=df_final[col], ax=axes[i], color='#132440', saturation=0.8)
    axes[i].set_title(f'Box Plot of {col} (Outliers Preserved)', fontsize=14, fontweight='bold')
    axes[i].grid(True, axis='y', alpha=0.5)

# Hide empty subplots
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.suptitle('Box Plots Showing Data Spread (Outliers Intentionally Kept)', fontsize=18, fontweight='bold', y=1.02)
plt.tight_layout(pad=3.0)
plt.show()

# 4. Pairplot with sampling for clarity (full data would be too dense)
sample_size = 12000
df_sample = df_final.sample(n=sample_size, random_state=42)

sns.pairplot(
    df_sample[numerical_features + ['is_promoted']],
    hue='is_promoted',
    palette={0: '#132440', 1: '#FDB5CE'},  # Your colors
    diag_kind='kde',
    plot_kws={'alpha': 0.6, 's': 30},
    corner=True  # Avoid duplication
)

plt.suptitle(f'Pairplot of Numerical Features (Sample of {sample_size:,} rows)\nColored by Promotion Status',
             fontsize=18, fontweight='bold', y=1.02)
plt.show()

# Final summary
print("\n=== Preprocessing Summary Complete ===")
print("• No multicollinearity (VIF < 2)")
print("• Outliers preserved (represent high performers/seniors)")
print("• Distributions analyzed and ready for modeling")
print("• Dataset is clean, balanced in features, and large (159k+ rows)")
print("→ Proceeding to modeling phase for optimal performance!")

"""**Identify rare categories**"""

print("Starting Rare Category Analysis & Handling...\n")

# Define categorical columns in your dataset
categorical_columns = ['department', 'region', 'education', 'gender', 'recruitment_channel']

# Remove columns that don't exist
categorical_columns = [col for col in categorical_columns if col in df.columns]

if not categorical_columns:
    print("No categorical columns found.")
else:
    print(f"Categorical columns to analyze: {categorical_columns}")
    print(f"Dataset size: {len(df):,} rows\n")

    # Threshold for rare categories (e.g., < 5% of data)
    threshold = 0.05  # 5% - you can change to 0.01 for stricter

    df_clean = df.copy()

    rare_summary = {}

    for col in categorical_columns:
        print(f"--- Analyzing column: '{col}' ---")

        # Calculate frequency (proportion)
        freq = df_clean[col].value_counts(normalize=True)

        # Identify rare categories
        rare_categories = freq[freq < threshold].index.tolist()

        rare_count = len(rare_categories)
        rare_percentage = freq[rare_categories].sum() * 100 if rare_count > 0 else 0

        print(f"  Total categories: {len(freq)}")
        print(f"  Rare categories (< {threshold*100:.0f}%): {rare_count}")
        if rare_count > 0:
            print(f"  Rare categories proportion: {rare_percentage:.2f}%")
            print(f"  Rare categories: {rare_categories}")

        # Handle rare categories: Group them into 'Other'
        if rare_count > 0:
            df_clean[col] = df_clean[col].apply(lambda x: 'Other' if x in rare_categories else x)
            print(f"  → Rare categories grouped into 'Other'")
            print(f"  New number of categories: {df_clean[col].nunique()}")
        else:
            print(f"  → No rare categories found. All categories are frequent.")

        rare_summary[col] = {
            'original_categories': len(freq),
            'rare_count': rare_count,
            'new_categories': df_clean[col].nunique(),
            'rare_proportion': rare_percentage
        }

        print()

    # Final summary
    print("="*60)
    print("Rare Category Handling Summary")
    total_rare_reduced = sum(info['original_categories'] - info['new_categories'] for info in rare_summary.values())
    print(f"  Total categories reduced across all columns: {total_rare_reduced}")
    print(f"  Final dataset shape: {df_clean.shape[0]:,} rows")

    # Update df to cleaned version
    df = df_clean

    print("\nRare category handling complete.")
    print("• Rare categories grouped into 'Other' to reduce cardinality")
    print("• This improves model stability and prevents overfitting on rare labels")
    print("• Dataset is now optimized for encoding (OneHot or Target encoding)")

# Optional: Show example after handling
if 'department' in df.columns:
    print("\nExample: Department categories after handling:")
    print(df['department'].value_counts())

"""# **Insights & Reporting**

**Feature importance (preliminary)**

# **Data Preprocessing**

**Encoding Categorical Variables**
"""

print(df.columns)

# columns One-Hot Encoding
categorical_cols = ['department', 'region', 'education', 'gender', 'recruitment_channel']

# One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# print first 5 rows after Encoding
print(df_encoded.head())

"""**Feature Scaling**"""

from sklearn.preprocessing import StandardScaler

numerical_cols = ['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'awards_won', 'avg_training_score']

scaler = StandardScaler()
df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])

# print first 5 rows after Scaling
print(df_encoded.head())

"""**Train–Test Split**"""

from sklearn.model_selection import train_test_split

# X = all features, y = target
X = df_encoded.drop('is_promoted', axis=1)
y = df_encoded['is_promoted']

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,       # 20% test
    random_state=42,
    stratify=y           # keep same class ratio
)

# Check distribution in train and test
print("Train distribution:")
print(y_train.value_counts())
print("\nTest distribution:")
print(y_test.value_counts())

"""**Handle Imbalanced Data**"""

from imblearn.over_sampling import SMOTE

# Apply SMOTE on training data only
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Check new distribution
print("Train distribution before SMOTE:")
print(y_train.value_counts())
print("\nTrain distribution after SMOTE:")
print(y_train_res.value_counts())

"""# **Build And Train Model**

**Pipeline Construction**
"""

print(df.columns)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline


# feature engineering
if 'age_group' not in df.columns:
    df['age_group'] = pd.cut(df['age'], bins=[0, 30, 40, 50, 60], labels=['<30', '30-40', '40-50', '>50'])

if 'high_training_score' not in df.columns:
    df['high_training_score'] = (df['avg_training_score'] > 80).astype(int)

# الميزة الجديدة المهمة: has_awards (بدل awards_won الأصلي)
df['has_awards'] = (df['awards_won'] == 1).astype(int)

# ميزة إضافية قوية جدًا
df['long_service_high_rating'] = ((df['length_of_service'] > 7) &
                                 (df['previous_year_rating'] >= 4)).astype(int)

# تحديد الميزات اللي هنستخدمها
numeric_features = [
    'no_of_trainings', 'age', 'previous_year_rating',
    'length_of_service', 'avg_training_score',
    'high_training_score', 'has_awards', 'long_service_high_rating'
]

categorical_features = [
    'department', 'region', 'education', 'gender',
    'recruitment_channel', 'age_group'
]

# نستبعد employee_id و awards_won الأصلي و is_promoted
features = numeric_features + categorical_features
X = df[features]
y = df['is_promoted']

# تقسيم البيانات مع الحفاظ على التوازن
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), numeric_features),

        ('cat', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))
        ]), categorical_features)
    ])

# XGBoost مع تعويض الـ class imbalance
model = XGBClassifier(
    random_state=42,
    eval_metric='aucpr',
    scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train)  # مهم جدًا
)

# Pipeline كامل مع SMOTE
pipeline = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', model)
])

# Grid Search بسيط وفعال
param_grid = {
    'classifier__n_estimators': [200, 300],
    'classifier__max_depth': [5, 7],
    'classifier__learning_rate': [0.05, 0.1]
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

grid_search = GridSearchCV(
    pipeline, param_grid, cv=cv, scoring='f1', n_jobs=-1, verbose=1
)

grid_search.fit(X_train, y_train)

# النتايج
best_model = grid_search.best_estimator_
print("Best parameters:", grid_search.best_params_)
print("Best CV F1 Score:", grid_search.best_score_)

# تقييم على Test
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

print("\n=== Test Set Performance ===")
print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_pred_proba))

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
pr_auc = auc(recall, precision)
print("Precision-Recall AUC:", pr_auc)

"""**Model Training & Hyperparameter Tuning**"""



import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from hyperopt.pyll.base import scope

# 1. تحويل الأعمدة الـ categorical إلى category dtype (ضروري)
categorical_cols = ['department', 'region', 'education', 'gender',
                    'recruitment_channel', 'age_group']

for col in categorical_cols:
    X_train[col] = X_train[col].astype('category')
    X_test[col] = X_test[col].astype('category')

print("Categorical columns converted to 'category' dtype ✓")

# 2. حساب scale_pos_weight للـ imbalance
scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()
print(f"scale_pos_weight: {scale_pos_weight:.2f}")

# 3. دالة الـ objective لـ Hyperopt
def objective(params):
    params['max_depth'] = int(params['max_depth'])
    params['n_estimators'] = int(params['n_estimators'])

    model = XGBClassifier(
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss',
        n_jobs=-1,
        scale_pos_weight=scale_pos_weight,
        enable_categorical=True,     # ← الحل للإيرور اللي كان بيطلع
        tree_method='hist',
        **params
    )

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = []

    for train_idx, val_idx in cv.split(X_train, y_train):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

        model.fit(X_tr, y_tr)
        pred = model.predict(X_val)
        scores.append(f1_score(y_val, pred))

    return {'loss': -np.mean(scores), 'status': STATUS_OK}

# 4. Search space (أفضل الباراميترز اللي بتفرق)
space = {
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),
    'max_depth': scope.int(hp.quniform('max_depth', 3, 10, 1)),
    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),
    'subsample': hp.uniform('subsample', 0.7, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1.0),
    'gamma': hp.uniform('gamma', 0, 5),
    'n_estimators': scope.int(hp.quniform('n_estimators', 200, 800, 50)),
}

# 5. تشغيل Hyperopt
print("\nStarting Hyperparameter Tuning with Hyperopt...")
trials = Trials()
best_params = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,  # 50 trial كافية جدًا لنتيجة قوية
    trials=trials,
    rstate=np.random.default_rng(42)
)

# تحويل القيم الصحيحة
best_params['max_depth'] = int(best_params['max_depth'])
best_params['n_estimators'] = int(best_params['n_estimators'])
best_params['min_child_weight'] = int(best_params['min_child_weight'])

print("\n=== Best Hyperparameters ===")
print(best_params)
print(f"Best CV F1 Score: {-trials.best_trial['result']['loss']:.4f}")

"""**final model**"""

# Training the final model with the best parameters
final_model = XGBClassifier(
    random_state=42,
    use_label_encoder=False,
    eval_metric='aucpr',
    n_jobs=-1,
    scale_pos_weight=scale_pos_weight,
    enable_categorical=True,
    tree_method='hist',
    **best_params
)

print("\nTraining final model...")
final_model.fit(X_train, y_train)

# Evaluation
y_pred = final_model.predict(X_test)
y_proba = final_model.predict_proba(X_test)[:, 1]

from sklearn.metrics import classification_report, f1_score, roc_auc_score, precision_recall_curve, auc

print("\n" + "="*60)
print("          FINAL TEST SET PERFORMANCE")
print("="*60)
print(classification_report(y_test, y_pred, digits=4))

print(f"F1 Score (promoted class): {f1_score(y_test, y_pred):.4f}")
print(f"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}")

precision, recall, _ = precision_recall_curve(y_test, y_proba)
pr_auc = auc(recall, precision)
print(f"PR AUC: {pr_auc:.4f}")

"""# **Model Evaluation if project Classification:**

**Key Metrics**
"""

from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, roc_auc_score, precision_recall_curve, auc, recall_score, precision_score
import matplotlib.pyplot as plt
import seaborn as sns

# Checking the predictions
y_pred = final_model.predict(X_test)
y_proba = final_model.predict_proba(X_test)[:, 1]

# Classification Report
print("\n" + "="*70)
print("                  CLASSIFICATION REPORT")
print("="*70)
print(classification_report(y_test, y_pred, digits=4))

# Key Metrics
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)  # ده اللي كان ناقص
roc_auc = roc_auc_score(y_test, y_proba)
precision, recall_curve, _ = precision_recall_curve(y_test, y_proba)
pr_auc = auc(recall_curve, precision)

print("\n" + "="*70)
print("                       KEY METRICS")
print("="*70)
print(f"Accuracy                  : {acc:.4f}")
print(f"F1-Score (promoted class) : {f1:.4f}")
print(f"Recall (promoted class)   : {recall:.4f}")
print(f"ROC-AUC Score             : {roc_auc:.4f}")
print(f"Precision-Recall AUC      : {pr_auc:.4f}")

"""**Confusion Matrix**"""

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Not Promoted', 'Promoted'],
            yticklabels=['Not Promoted', 'Promoted'])
plt.title('Confusion Matrix', fontsize=16)
plt.ylabel('Actual', fontsize=14)
plt.xlabel('Predicted', fontsize=14)
plt.tight_layout()
plt.show()

"""**ROC Curve & Precision-Recall Curve**"""

from sklearn.metrics import roc_curve

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})', color='deepskyblue', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.title('ROC-AUC Curve', fontsize=16)
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print("\n\n\n")

# Precision-Recall Curve (مع تصليح الـ shapes)
precision, recall, _ = precision_recall_curve(y_test, y_proba)

# التأكد من الأشكال صح (ravel لو لازم)
precision = np.ravel(precision)
recall = np.ravel(recall)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})', color='green', linewidth=2)
plt.title('Precision-Recall Curve', fontsize=16)
plt.xlabel('Recall', fontsize=14)
plt.ylabel('Precision', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.tight_layout()
plt.show()

"""**Feature Importance**"""

# Feature Importance
importances = final_model.feature_importances_
feature_names = X_train.columns

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values(by='importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=importance_df.head(15), x='importance', y='feature', palette='viridis')
plt.title('Top 15 Feature Importances (XGBoost)', fontsize=16)
plt.xlabel('Importance')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

print("Top 10 Important Features:")
print(importance_df.head(10))

"""# **Save Deployment Model In Streamlit**

**step 1**
"""


import joblib

# Saving the final model (the one trained on 300,000 lines)
joblib.dump(final_model, 'employee_promotion_model.pkl')

# Download the file to your device
from google.colab import files
files.download('employee_promotion_model.pkl')
